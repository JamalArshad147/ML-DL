{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12788d45",
   "metadata": {},
   "source": [
    "## Implementing a ChatGPT App with LangChain from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60530a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'hello g kia hal haio?', 'text': 'Hallo! Mir geht es gut, danke. Wie kann ich Ihnen helfen?'}\n",
      "--------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True) \n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_classic.chains import LLMChain\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\"],\n",
    "    messages=[\n",
    "#         SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
    "        SystemMessage(content='You respond only in German.'),\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "while True:\n",
    "    content = input('Your prompt: ')\n",
    "    if content.lower() in ['quit', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "    \n",
    "    response = chain.invoke({'content': content})\n",
    "    print(response)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370c0b7",
   "metadata": {},
   "source": [
    "## Adding Chat Memory Using ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a3e2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=False) \n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "# 1. Imports\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "# 2. Create memory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# 3. add  MessagesPlaceholder(variable_name='messages') to the prompt\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\", \"chat_history\"],\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), # Where the memory will be stored.\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Add the memory to the chain\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "while True:\n",
    "    content = input('Your prompt: ')\n",
    "    if content.lower() in ['quit', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "    \n",
    "    response = chain.invoke({'content': content})\n",
    "    print(response)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28677aa0",
   "metadata": {},
   "source": [
    "## Saving Chat Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723b59ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': 'hello g kia hal hai', 'chat_history': [], 'text': 'Hello! Main theek hoon, shukriya. Aap kaise hain?'}\n",
      "--------------------------------------------------\n",
      "{'content': 'mai theek or sunao kesi guzr rahi?', 'chat_history': [HumanMessage(content='hello g kia hal hai', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello! Main theek hoon, shukriya. Aap kaise hain?', additional_kwargs={}, response_metadata={})], 'text': 'Mai bhi theek hoon, shukriya. Aapne kuch interesting kiya ya dekha aaj kal?'}\n",
      "--------------------------------------------------\n",
      "{'content': 'hassan pagal hai mera dost uska kia kru', 'chat_history': [HumanMessage(content='hello g kia hal hai', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello! Main theek hoon, shukriya. Aap kaise hain?', additional_kwargs={}, response_metadata={}), HumanMessage(content='mai theek or sunao kesi guzr rahi?', additional_kwargs={}, response_metadata={}), AIMessage(content='Mai bhi theek hoon, shukriya. Aapne kuch interesting kiya ya dekha aaj kal?', additional_kwargs={}, response_metadata={})], 'text': 'Agar aapka dost Hassan ko lagta hai ki woh pagal hai, to shayad usse achhe se baat karke uska mood samjha ja sakta hai. Pyaar aur samajh se hi kisi dost ko samjha ja sakta hai. Aap usse uske feelings share karne ke liye encourage kar sakte hain. Kabhi kabhi humein apne dosto ki madad karne ki zarurat hoti hai.'}\n",
      "--------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=False) \n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "# Messages and Prompts come from 'core'\n",
    "from langchain_core.messages import SystemMessage\n",
    "from langchain_core.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "\n",
    "# Legacy chains and memory come from 'classic'\n",
    "from langchain_classic.chains import LLMChain\n",
    "from langchain_classic.memory import ConversationBufferMemory, FileChatMessageHistory\n",
    "\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "# 2. Add an additional keyword argument to the ConversationBufferMemory() constructor\n",
    "history = FileChatMessageHistory('chat_history.json')\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='chat_history',\n",
    "    chat_memory=history,\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\", \"chat_history\"],\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"), \n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "while True:\n",
    "    content = input('Your prompt: ')\n",
    "    if content.lower() in ['quit', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "    \n",
    "    response = chain.invoke({'content': content})\n",
    "    print(response)\n",
    "    print('-' * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42074641",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='hello g kia hal hai', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello! Main theek hoon, shukriya. Aap kaise hain?', additional_kwargs={}, response_metadata={}), HumanMessage(content='mai theek or sunao kesi guzr rahi?', additional_kwargs={}, response_metadata={}), AIMessage(content='Mai bhi theek hoon, shukriya. Aapne kuch interesting kiya ya dekha aaj kal?', additional_kwargs={}, response_metadata={}), HumanMessage(content='hassan pagal hai mera dost uska kia kru', additional_kwargs={}, response_metadata={}), AIMessage(content='Agar aapka dost Hassan ko lagta hai ki woh pagal hai, to shayad usse achhe se baat karke uska mood samjha ja sakta hai. Pyaar aur samajh se hi kisi dost ko samjha ja sakta hai. Aap usse uske feelings share karne ke liye encourage kar sakte hain. Kabhi kabhi humein apne dosto ki madad karne ki zarurat hoti hai.', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "# The messages property contains the list of messages in order.\n",
    "print(history.messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cfdd50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
