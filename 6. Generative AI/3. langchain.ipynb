{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e372b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c9671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat Models\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "output = llm.invoke(\"Explain Agentic AI...\")\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"you are a poet\"),\n",
    "    HumanMessage(content=\"Write four lines on winter winds, coding, moon and coffee\"),\n",
    "]\n",
    "\n",
    "output = llm.invoke(messages)\n",
    "print(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a0050fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caching LLM Responses\n",
    "\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.cache import InMemoryCache\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "set_llm_cache(InMemoryCache())\n",
    "prompt = \"Write 4 lines on Nature and Winter winds\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86193a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fe4947",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "llm.invoke(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723fb346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Streaming\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\")\n",
    "prompt = \"Write poem on code\"\n",
    "\n",
    "for chunk in llm.stream(prompt):\n",
    "    if chunk is not None:\n",
    "        print(chunk.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a89b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PromptTemplates\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "template = \"\"\"You are an experienced Virologist. Write a few senteces about the following virus {virus} in {language}\"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(template=template)\n",
    "prompt = prompt_template.format(virus=\"HTLV-1\", language=\"English\")\n",
    "print(prompt)\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "llm.invoke(prompt).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5316c9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define role msg\n",
    "# Cache\n",
    "# Streaming\n",
    "# PromptTemplate\n",
    "\n",
    "import time\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.globals import set_llm_cache\n",
    "from langchain_community.cache import InMemoryCache\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "\n",
    "template = \"\"\"You are an experienced AI engineer with over 2 decades of experience. Explain {user_prompt} in simple words in 3 lines max and output shouldn't be just one long line\"\"\"\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \").strip().lower()\n",
    "\n",
    "    if user_input.lower() not in [\"bye\", \"exit\", \"stop\"]:\n",
    "\n",
    "        prompt = PromptTemplate.from_template(template).format(user_prompt=user_input)\n",
    "\n",
    "        start_time = time.time()\n",
    "        print(\"PROMPT: \", prompt)\n",
    "        print(\"----------------------------\")\n",
    "        # for chunk in llm.stream(prompt):\n",
    "        #     if (chunk is not None ):\n",
    "        #         print(chunk.content, end=\"\")\n",
    "\n",
    "        llm.invoke(prompt)\n",
    "\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"\\n\\n[Generation took {elapsed_time:.2f} seconds]\\n\")\n",
    "\n",
    "    else:\n",
    "        print(\"Saving...\")\n",
    "        time.sleep(4)\n",
    "        print(\"Bye bye!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51428c1a",
   "metadata": {},
   "source": [
    "## Chaining in Langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f916bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic.chains import LLMChain, SimpleSequentialChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd607aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_1 = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0.5)\n",
    "\n",
    "prompt_template_1 = PromptTemplate.from_template(\n",
    "    template=\"You are an experienced AI Scientist and Python developer, Write a function on {concept} with details and coding examples\"\n",
    ")\n",
    "\n",
    "chain_1 = LLMChain(llm=llm_1, prompt=prompt_template_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237ecde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_2 = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1.5)\n",
    "\n",
    "prompt_template_2 = PromptTemplate.from_template(\n",
    "    template=\"Given the python {function} describe in details.\"\n",
    ")\n",
    "\n",
    "chain_2 = LLMChain(llm=llm_2, prompt=prompt_template_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db6e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chains = SimpleSequentialChain(chains=[chain_1, chain_2], verbose=True)\n",
    "output = chains.invoke(\"Machine Learning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbbc759",
   "metadata": {},
   "source": [
    "## Langchain Agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb24cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade langchain-core langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639d2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install --upgrade langchain langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463576fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install langchain-experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aedd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "# python_repl = PythonREPL()\n",
    "# python_repl.run('print([n for n in range(1,100) if n%13==0])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee542f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_experimental.agents.agent_toolkits import create_python_agent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
